{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"seed\": 42,\n",
    "    \"data_dir\": \"data/\",\n",
    "    \"train_dir\": \"outputs/multi_task_model\",\n",
    "    \"model_file\": \"outputs/model\",\n",
    "    \"num_past_utterances\": 1,\n",
    "    \"num_future_utterances\": 1,\n",
    "    \"epoch\": 5,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 8,\n",
    "    \"do_train\": True,\n",
    "    \"evaluation\": \"emotion\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch.nn as nn\n",
    "from datasets import ClassLabel, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import Trainer\n",
    "from transformers import is_datasets_available\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2020 CLTL Leolani\n",
    "Copyright (c) 2021 Junyin Chen\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MELD_SPEAKER = [\"Chandler\", \"Joey\", \"Monica\", \"Rachel\", \"Ross\", \"Phoebe\"]\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seed to a fixed value.\n",
    "\n",
    "    Set everything to be deterministic\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def get_emotion2id(dataset: str) -> Tuple[dict, dict]:\n",
    "    \"\"\"Get a dict that converts string class to number.\"\"\"\n",
    "\n",
    "    if dataset == \"MELD\":\n",
    "        # MELD has 7 classes\n",
    "        emotions = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "        emotion2id = {emotion: idx for idx, emotion in enumerate(emotions)}\n",
    "        id2emotion = {val: key for key, val in emotion2id.items()}\n",
    "    return emotion2id, id2emotion\n",
    "\n",
    "\n",
    "def get_speaker2id(dataset: str) -> Tuple[dict, dict]:\n",
    "    \"\"\"Get a dict that converts string class to number.\"\"\"\n",
    "\n",
    "    if dataset == \"MELD\":\n",
    "        # MELD has 7 classes\n",
    "        speakers = [\n",
    "            \"CHANDLER\",\n",
    "            \"JOEY\",\n",
    "            \"MONICA\",\n",
    "            \"RACHEL\",\n",
    "            \"ROSS\",\n",
    "            \"PHOEBE\",\n",
    "            \"OTHERS\",\n",
    "        ]\n",
    "        speaker2id = {speaker: idx for idx, speaker in enumerate(speakers)}\n",
    "        id2speaker = {val: key for key, val in speaker2id.items()}\n",
    "    return speaker2id, id2speaker\n",
    "\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset=\"MELD\",\n",
    "            split=\"train\",\n",
    "            field=\"emotion\",\n",
    "            num_past_utterances=0,\n",
    "            num_future_utterances=0,\n",
    "            model_checkpoint=\"roberta-base\",\n",
    "            directory=\"data/\",\n",
    "            up_to=False,\n",
    "            seed=0\n",
    "    ):\n",
    "        \"\"\"Initialize emotion recognition in conversation text modality dataset class.\"\"\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.directory = directory\n",
    "        self.split = split\n",
    "        self.field = field\n",
    "        self.num_past_utterances = num_past_utterances\n",
    "        self.num_future_utterances = num_future_utterances\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.emotion2id, self.id2emotion = get_emotion2id(self.dataset)\n",
    "        self.speaker2id, self.id2speaker = get_speaker2id(self.dataset)\n",
    "        self.up_to = up_to\n",
    "        self.seed = seed\n",
    "\n",
    "        self._load_from_raw()\n",
    "        self._string2tokens()\n",
    "\n",
    "    def _load_from_raw(self):\n",
    "        if self.dataset in [\"MELD\"]:\n",
    "            # Get the full file path\n",
    "            raw_path = os.path.join(self.directory, self.dataset, self.split + \"_sent_emo.csv\")\n",
    "            # Load csv to pandas Dataframe\n",
    "            raw_data = pd.read_csv(raw_path)\n",
    "            # Load each field\n",
    "            self.dialog_group = {}\n",
    "            self.emotion = {}\n",
    "            self.speaker_emotion = {}\n",
    "            for _, row in raw_data.iterrows():\n",
    "                utterance_id = '{}_{}_{}_{}'.format(row['Dialogue_ID'],\n",
    "                                                    row['Utterance_ID'],\n",
    "                                                    row['Season'],\n",
    "                                                    row['Episode'])\n",
    "                dialog_id = '{}_{}_{}'.format(row['Dialogue_ID'], row['Season'], row['Episode'])\n",
    "                if dialog_id not in self.dialog_group:\n",
    "                    self.dialog_group[dialog_id] = []\n",
    "                self.dialog_group[dialog_id].append(utterance_id)\n",
    "                self.emotion[utterance_id] = row['Emotion']\n",
    "                utterance = row[\"Utterance\"]\n",
    "                speaker = \"OTHERS\"\n",
    "                if row[\"Speaker\"] in MELD_SPEAKER:\n",
    "                    speaker = row[\"Speaker\"].upper()\n",
    "\n",
    "                self.speaker_emotion[utterance_id] = {\"utterance\": speaker + \": \" + utterance,\n",
    "                                                      \"clean\": utterance,\n",
    "                                                      \"emotion\": row['Emotion'],\n",
    "                                                      \"speaker\": speaker,\n",
    "                                                      }\n",
    "        else:\n",
    "            raise ValueError(f\"{self.dataset} is not MELD\")\n",
    "\n",
    "    def _create_input(\n",
    "            self, diaids, num_past_utterances, num_future_utterances\n",
    "    ):\n",
    "        \"\"\"Create an input which will be an input to RoBERTa.\"\"\"\n",
    "\n",
    "        args = {\n",
    "            \"diaids\": diaids,\n",
    "            \"num_past_utterances\": num_past_utterances,\n",
    "            \"num_future_utterances\": num_future_utterances,\n",
    "        }\n",
    "\n",
    "        logging.debug(f\"arguments given: {args}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint, use_fast=True)\n",
    "        max_model_input_size = tokenizer.max_model_input_sizes[self.model_checkpoint]\n",
    "        num_truncated = 0\n",
    "\n",
    "        inputs = []\n",
    "        for diaid in tqdm(diaids):\n",
    "            ues = [\n",
    "                self.speaker_emotion[uttid]\n",
    "                for uttid in self.dialog_group[diaid]\n",
    "            ]\n",
    "\n",
    "            num_tokens = [len(tokenizer(ue[\"utterance\"])[\"input_ids\"]) for ue in ues]\n",
    "\n",
    "            for idx, ue in enumerate(ues):\n",
    "                if ue[\"emotion\"] not in list(self.emotion2id.keys()):\n",
    "                    continue\n",
    "\n",
    "                emotion = self.emotion2id[ue[\"emotion\"]]\n",
    "                speaker = self.speaker2id[ue[\"speaker\"]]\n",
    "\n",
    "                indexes = [idx]\n",
    "                indexes_past = [\n",
    "                    i for i in range(idx - 1, idx - num_past_utterances - 1, -1)\n",
    "                ]\n",
    "                indexes_future = [\n",
    "                    i for i in range(idx + 1, idx + num_future_utterances + 1, 1)\n",
    "                ]\n",
    "\n",
    "                offset = 0\n",
    "                if len(indexes_past) < len(indexes_future):\n",
    "                    for _ in range(len(indexes_future) - len(indexes_past)):\n",
    "                        indexes_past.append(None)\n",
    "                elif len(indexes_past) > len(indexes_future):\n",
    "                    for _ in range(len(indexes_past) - len(indexes_future)):\n",
    "                        indexes_future.append(None)\n",
    "\n",
    "                for i, j in zip(indexes_past, indexes_future):\n",
    "                    if i is not None and i >= 0:\n",
    "                        indexes.insert(0, i)\n",
    "                        offset += 1\n",
    "                        if (\n",
    "                                sum([num_tokens[idx_] for idx_ in indexes])\n",
    "                                > max_model_input_size\n",
    "                        ):\n",
    "                            del indexes[0]\n",
    "                            offset -= 1\n",
    "                            num_truncated += 1\n",
    "                            break\n",
    "                    if j is not None and j < len(ues):\n",
    "                        indexes.append(j)\n",
    "                        if (\n",
    "                                sum([num_tokens[idx_] for idx_ in indexes])\n",
    "                                > max_model_input_size\n",
    "                        ):\n",
    "                            del indexes[-1]\n",
    "                            num_truncated += 1\n",
    "                            break\n",
    "\n",
    "                utterances = [ues[idx_][\"utterance\"] for idx_ in indexes]\n",
    "                clean = [ues[idx_][\"clean\"] for idx_ in indexes]\n",
    "\n",
    "                if num_past_utterances == 0 and num_future_utterances == 0:\n",
    "                    assert len(utterances) == 1\n",
    "                    final_utterance = clean[0]\n",
    "\n",
    "                elif num_past_utterances > 0 and num_future_utterances == 0:\n",
    "                    if len(utterances) == 1:\n",
    "                        final_utterance = \"</s></s>\" + clean[-1]\n",
    "                    else:\n",
    "                        final_utterance = (\n",
    "                                \" \".join(utterances[:-1]) + \"</s></s>\" + clean[-1]\n",
    "                        )\n",
    "\n",
    "                elif num_past_utterances == 0 and num_future_utterances > 0:\n",
    "                    if len(utterances) == 1:\n",
    "                        final_utterance = clean[0] + \"</s></s>\"\n",
    "                    else:\n",
    "                        final_utterance = (\n",
    "                                clean[0] + \"</s></s>\" + \" \".join(utterances[1:])\n",
    "                        )\n",
    "\n",
    "                elif num_past_utterances > 0 and num_future_utterances > 0:\n",
    "                    if len(utterances) == 1:\n",
    "                        final_utterance = \"</s></s>\" + clean[0] + \"</s></s>\"\n",
    "                    else:\n",
    "                        final_utterance = (\n",
    "                                \" \".join(utterances[:offset])\n",
    "                                + \"</s></s>\"\n",
    "                                + clean[offset]\n",
    "                                + \"</s></s>\"\n",
    "                                + \" \".join(utterances[offset + 1:])\n",
    "                        )\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "                input_ids_attention_mask = tokenizer(final_utterance)\n",
    "                input_ids = input_ids_attention_mask[\"input_ids\"]\n",
    "                attention_mask = input_ids_attention_mask[\"attention_mask\"]\n",
    "\n",
    "                label = emotion\n",
    "                if self.field == \"speaker\":\n",
    "                    label = speaker\n",
    "\n",
    "                input_ = {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"label\": label,\n",
    "                }\n",
    "\n",
    "                inputs.append(input_)\n",
    "\n",
    "        logging.info(f\"number of truncated utterances: {num_truncated}\")\n",
    "        return inputs\n",
    "\n",
    "    def _string2tokens(self):\n",
    "        \"\"\"Convert string to (BPE) tokens.\"\"\"\n",
    "        logging.info(f\"converting utterances into tokens ...\")\n",
    "\n",
    "        diaids = sorted(list(self.dialog_group.keys()))\n",
    "\n",
    "        set_seed(self.seed)\n",
    "        random.shuffle(diaids)\n",
    "\n",
    "        if self.up_to:\n",
    "            logging.info(f\"Using only the first {self.up_to} dialogues ...\")\n",
    "            diaids = diaids[: self.up_to]\n",
    "\n",
    "        logging.info(f\"creating input utterance data ... \")\n",
    "        self.inputs_ = self._create_input(\n",
    "            diaids=diaids,\n",
    "            num_past_utterances=self.num_past_utterances,\n",
    "            num_future_utterances=self.num_future_utterances,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs_[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###MTL/data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SingleTaskDataLoader:\n",
    "    def __init__(self, task, **kwargs):\n",
    "        self.task = task\n",
    "        self.data_loader = DataLoader(**kwargs)\n",
    "        self.batch_size = self.data_loader.batch_size\n",
    "        self.dataset = self.data_loader.dataset\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task\"] = self.task\n",
    "            yield batch\n",
    "    \n",
    "class MultiTaskDataLoader:\n",
    "    def __init__(self, task_data_loaders):\n",
    "        self.task_data_loaders = task_data_loaders\n",
    "        self.dataset = [None] * sum([len(dl.dataset) for dl in task_data_loaders.values()])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return sum([len(dl) for dl in self.task_data_loaders.values()])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        task_choices = []\n",
    "        for task, dl in self.task_data_loaders.items():\n",
    "            task_choices.extend([task] * len(dl))\n",
    "        task_choices = np.array(task_choices)\n",
    "        np.random.shuffle(task_choices)\n",
    "        for task in task_choices:\n",
    "            yield next(iter(self.task_data_loaders[task]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###MTL/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiTaskModel(PreTrainedModel):\n",
    "    def __init__(self, encoder, task_models):\n",
    "        super(MultiTaskModel, self).__init__(PretrainedConfig())\n",
    "        self.encoder = encoder\n",
    "        self.task_models = nn.ModuleDict(task_models)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_task_models(cls, task_models):\n",
    "        shared_encoder = None\n",
    "        for model in task_models.values():\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n",
    "            else:\n",
    "                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n",
    "        return cls(shared_encoder, task_models)\n",
    "                  \n",
    "    @staticmethod\n",
    "    def get_encoder_attr_name(model):\n",
    "        model_name = model.__class__.__name__\n",
    "        if model_name.startswith('Bert'):\n",
    "            return 'bert'\n",
    "        elif model_name.startswith('Roberta'):\n",
    "            return 'roberta'\n",
    "        elif model_name.startswith('Albert'):\n",
    "            return 'albert'\n",
    "        else:\n",
    "            raise ValueError('Unsupported model: {}'.format(model_name))\n",
    "        \n",
    "    def forward(self, task, input_ids, attention_mask, **kwargs):\n",
    "        model = self.task_models[task]\n",
    "        return model(input_ids, attention_mask, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###MLT/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiTaskTrainer(Trainer):\n",
    "\n",
    "    def get_single_task_dataloader(self, task, dataset, description):\n",
    "        if description == \"training\" and self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        elif description == \"evaluation\" and dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "\n",
    "        if is_datasets_available() and isinstance(dataset, datasets.Dataset):\n",
    "            dataset = self._remove_unused_columns(dataset, description=description)\n",
    "\n",
    "        if isinstance(dataset, torch.utils.data.IterableDataset):\n",
    "            if self.args.world_size > 1:\n",
    "                dataset = IterableDatasetShard(\n",
    "                    dataset,\n",
    "                    batch_size=self.args.train_batch_size,\n",
    "                    drop_last=self.args.dataloader_drop_last,\n",
    "                    num_processes=self.args.world_size,\n",
    "                    process_index=self.args.process_index,\n",
    "                )\n",
    "\n",
    "            return SingleTaskDataLoader(\n",
    "                task,\n",
    "                dataset=dataset,\n",
    "                batch_size=self.args.per_device_train_batch_size,\n",
    "                collate_fn=self.data_collator,\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "\n",
    "        if description == \"training\":\n",
    "            self.train_dataset, dataset = dataset, self.train_dataset\n",
    "            sampler = self._get_train_sampler()\n",
    "            self.train_dataset, dataset = dataset, self.train_dataset\n",
    "            batch_size = self.args.train_batch_size\n",
    "        else:\n",
    "            sampler = self._get_eval_sampler(dataset)\n",
    "            batch_size = self.args.eval_batch_size\n",
    "\n",
    "        return SingleTaskDataLoader(\n",
    "            task,\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        return MultiTaskDataLoader({\n",
    "            task: self.get_single_task_dataloader(task, dataset, description=\"training\")\n",
    "            for task, dataset in self.train_dataset.items()\n",
    "        })\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        task_to_eval = eval_dataset[\"task\"]\n",
    "        return self.get_single_task_dataloader(task_to_eval, eval_dataset[task_to_eval], description=\"evaluation\")\n",
    "\n",
    "    def get_test_dataloader(self, test_dataset):\n",
    "        task_to_test = test_dataset[\"task\"]\n",
    "        return self.get_single_task_dataloader(task_to_test, test_dataset[task_to_test], description=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    \"emotion\": TextDataset(\n",
    "        dataset=\"MELD\",\n",
    "        split=\"train\",\n",
    "        field=\"emotion\",\n",
    "        seed=args[\"seed\"],\n",
    "        directory=args[\"data_dir\"],\n",
    "        num_past_utterances=args[\"num_past_utterances\"],\n",
    "        num_future_utterances=args[\"num_future_utterances\"],\n",
    "    ),\n",
    "    \"speaker\": TextDataset(\n",
    "        dataset=\"MELD\",\n",
    "        split=\"train\",\n",
    "        field=\"speaker\",\n",
    "        seed=args[\"seed\"],\n",
    "        directory=args[\"data_dir\"],\n",
    "        num_past_utterances=args[\"num_past_utterances\"],\n",
    "        num_future_utterances=args[\"num_future_utterances\"],\n",
    "    )\n",
    "}\n",
    "eval_dataset = {\"emotion\": TextDataset(\n",
    "    dataset=\"MELD\",\n",
    "    split=\"dev\",\n",
    "    field=\"emotion\",\n",
    "    seed=args[\"seed\"],\n",
    "    directory=args[\"data_dir\"],\n",
    "    num_past_utterances=args[\"num_past_utterances\"],\n",
    "    num_future_utterances=args[\"num_future_utterances\"],\n",
    "), \"speaker\": TextDataset(\n",
    "    dataset=\"MELD\",\n",
    "    split=\"dev\",\n",
    "    field=\"speaker\",\n",
    "    seed=args[\"seed\"],\n",
    "    directory=args[\"data_dir\"],\n",
    "    num_past_utterances=args[\"num_past_utterances\"],\n",
    "    num_future_utterances=args[\"num_future_utterances\"],\n",
    "), \"task\": args[\"evaluation\"]}\n",
    "\n",
    "test_dataset = {\"emotion\": TextDataset(\n",
    "    dataset=\"MELD\",\n",
    "    split=\"test\",\n",
    "    field=\"emotion\",\n",
    "    seed=args[\"seed\"],\n",
    "    directory=args[\"data_dir\"],\n",
    "    num_past_utterances=args[\"num_past_utterances\"],\n",
    "    num_future_utterances=args[\"num_future_utterances\"],\n",
    "), \"speaker\": TextDataset(\n",
    "    dataset=\"MELD\",\n",
    "    split=\"test\",\n",
    "    field=\"speaker\",\n",
    "    seed=args[\"seed\"],\n",
    "    directory=args[\"data_dir\"],\n",
    "    num_past_utterances=args[\"num_past_utterances\"],\n",
    "    num_future_utterances=args[\"num_future_utterances\"],\n",
    "), \"task\": args[\"evaluation\"]}\n",
    "\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "labels = {\n",
    "    \"speaker\": ClassLabel(num_classes=7,\n",
    "                          names=[\"Chandler\", \"Joey\", \"Monica\", \"Rachel\", \"Ross\", \"Phoebe\", \"Others\"]),\n",
    "    \"emotion\": ClassLabel(num_classes=7,\n",
    "                          names=[\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"])\n",
    "}\n",
    "single_task_models = {\n",
    "    task: AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=label.num_classes)\n",
    "    for task, label in labels.items()\n",
    "}\n",
    "multi_task_model = MultiTaskModel.from_task_models(single_task_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not args[\"do_train\"]:\n",
    "    multi_task_model.load_state_dict(\n",
    "        torch.load(args[\"model_file\"], map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_task_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624,
     "referenced_widgets": [
      "f5876590064446d98b0e78d28e0e5cc2",
      "471a922b05c048bd90359a03a72de82e",
      "6c0d8b279c0e442c8e241ce0822156e0",
      "a24c1c7229ce4b54be0df61081ecde00",
      "70ef47f9c26c4585ab9a85e43dee48a6",
      "5d63a455ce5046d491b0291fc4d536a4",
      "13f124717e3a46898289ae9aa2bad191",
      "b02467e58ce04062b3f87628b8291b43",
      "99cda41eb08843d780f5825386be0d05",
      "434f1308b48340a9bbebcc9884ffcbe1",
      "e74fb905d22f46c1bcde03739301e7d0"
     ]
    },
    "id": "2gazbzYk_yKX",
    "outputId": "dc86136c-179e-43ac-99b1-a9861566eb1d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args[\"train_dir\"],\n",
    "    overwrite_output_dir=True,\n",
    "    label_names=[\"labels\"],\n",
    "    learning_rate=args[\"learning_rate\"],\n",
    "    num_train_epochs=args[\"epoch\"],\n",
    "    per_device_train_batch_size=args[\"batch_size\"],  \n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    multi_task_model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "if args[\"do_train\"]:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUM1PbI0__Nn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f1 = trainer.predict(test_dataset).metrics['test_f1']\n",
    "print(\"Weighted F1:\", f1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DmpSNfMXW8AW",
    "Y7LZ98xqW3sk",
    "3vu9ks2HWodt"
   ],
   "name": "D2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13f124717e3a46898289ae9aa2bad191": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "434f1308b48340a9bbebcc9884ffcbe1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "471a922b05c048bd90359a03a72de82e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d63a455ce5046d491b0291fc4d536a4",
      "placeholder": "​",
      "style": "IPY_MODEL_13f124717e3a46898289ae9aa2bad191",
      "value": "Downloading builder script: "
     }
    },
    "5d63a455ce5046d491b0291fc4d536a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c0d8b279c0e442c8e241ce0822156e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b02467e58ce04062b3f87628b8291b43",
      "max": 2059,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_99cda41eb08843d780f5825386be0d05",
      "value": 2059
     }
    },
    "70ef47f9c26c4585ab9a85e43dee48a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99cda41eb08843d780f5825386be0d05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a24c1c7229ce4b54be0df61081ecde00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_434f1308b48340a9bbebcc9884ffcbe1",
      "placeholder": "​",
      "style": "IPY_MODEL_e74fb905d22f46c1bcde03739301e7d0",
      "value": " 5.27k/? [00:00&lt;00:00, 152kB/s]"
     }
    },
    "b02467e58ce04062b3f87628b8291b43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e74fb905d22f46c1bcde03739301e7d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5876590064446d98b0e78d28e0e5cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_471a922b05c048bd90359a03a72de82e",
       "IPY_MODEL_6c0d8b279c0e442c8e241ce0822156e0",
       "IPY_MODEL_a24c1c7229ce4b54be0df61081ecde00"
      ],
      "layout": "IPY_MODEL_70ef47f9c26c4585ab9a85e43dee48a6"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
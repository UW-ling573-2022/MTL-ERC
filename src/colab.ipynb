{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"seed\": 42,\n",
    "    \"data_dir\": \"data/\",\n",
    "    \"train_dir\": \"outputs/multi_task_model\",\n",
    "    \"model_file\": \"outputs/pytorch_model.bin\",\n",
    "    \"num_past_utterances\": 6,\n",
    "    \"num_future_utterances\": 0,\n",
    "    \"speaker_in_context\": False,\n",
    "    \"epoch\": 5,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"do_train\": True,\n",
    "    \"checkpoint\": \"roberta-base\",\n",
    "    \"training\": [\"Emotion\"],\n",
    "    \"evaluation\": \"Emotion\",\n",
    "    'output_file': 'outputs/predictions.out',\n",
    "    \"result_file\": \"results/scores.out\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "if kwargs[\"do_train\"]:\n",
    "  drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/MELD\n",
    "!cd data/MELD && wget https://raw.githubusercontent.com/UW-ling573-2022/data/main/MELD/dev_sent_emo.csv\n",
    "!cd data/MELD && wget https://raw.githubusercontent.com/UW-ling573-2022/data/main/MELD/test_sent_emo.csv\n",
    "!cd data/MELD && wget https://raw.githubusercontent.com/UW-ling573-2022/data/main/MELD/train_sent_emo.csv\n",
    "!mkdir -p data/EMORYNLP\n",
    "!cd data/EMORYNLP && wget https://raw.githubusercontent.com/UW-ling573-2022/data/main/EMORYNLP/dev.csv\n",
    "!cd data/EMORYNLP && wget https://raw.githubusercontent.com/UW-ling573-2022/data/main/EMORYNLP/train.csv\n",
    "!cd data/EMORYNLP && wget https://raw.githubusercontent.com/UW-ling573-2022/data/main/EMORYNLP/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir -p outputs\n",
    "import gdown\n",
    "if not kwargs[\"do_train\"]:\n",
    "  url = \"https://drive.google.com/uc?id=10p4r_L1NYBHCQ3M-bdXucmDmQI2fa3YW\"\n",
    "  gdown.download(url, output=kwargs[\"model_file\"], quiet=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import ClassLabel, load_metric, Dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import Trainer\n",
    "from transformers import is_datasets_available\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###MTL/data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SingleTaskDataLoader:\n",
    "    def __init__(self, task, **kwargs):\n",
    "        self.task = task\n",
    "        self.data_loader = DataLoader(**kwargs)\n",
    "        self.batch_size = self.data_loader.batch_size\n",
    "        self.dataset = self.data_loader.dataset\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task\"] = self.task\n",
    "            yield batch\n",
    "    \n",
    "class MultiTaskDataLoader:\n",
    "    def __init__(self, task_data_loaders):\n",
    "        self.task_data_loaders = task_data_loaders\n",
    "        self.dataset = [None] * sum([len(dl.dataset) for dl in task_data_loaders.values()])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return sum([len(dl) for dl in self.task_data_loaders.values()])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        task_choices = []\n",
    "        for task, dl in self.task_data_loaders.items():\n",
    "            task_choices.extend([task] * len(dl))\n",
    "        task_choices = np.array(task_choices)\n",
    "        np.random.shuffle(task_choices)\n",
    "        for task in task_choices:\n",
    "            yield next(iter(self.task_data_loaders[task]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###MTL/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiTaskModel(PreTrainedModel):\n",
    "    def __init__(self, encoder, task_models):\n",
    "        super(MultiTaskModel, self).__init__(PretrainedConfig())\n",
    "        self.encoder = encoder\n",
    "        self.task_models = nn.ModuleDict(task_models)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_task_models(cls, task_models):\n",
    "        shared_encoder = None\n",
    "        for model in task_models.values():\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n",
    "            else:\n",
    "                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n",
    "        return cls(shared_encoder, task_models)\n",
    "                  \n",
    "    @staticmethod\n",
    "    def get_encoder_attr_name(model):\n",
    "        model_name = model.__class__.__name__\n",
    "        if model_name.startswith('Bert'):\n",
    "            return 'bert'\n",
    "        elif model_name.startswith('Roberta'):\n",
    "            return 'roberta'\n",
    "        elif model_name.startswith('Albert'):\n",
    "            return 'albert'\n",
    "        else:\n",
    "            raise ValueError('Unsupported model: {}'.format(model_name))\n",
    "        \n",
    "    def forward(self, task, input_ids, attention_mask, **kwargs):\n",
    "        model = self.task_models[task]\n",
    "        return model(input_ids, attention_mask, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###MLT/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiTaskTrainer(Trainer):\n",
    "\n",
    "    def get_single_task_dataloader(self, task, dataset, description):\n",
    "        if description == \"training\" and self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        elif description == \"evaluation\" and dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "\n",
    "        if is_datasets_available() and isinstance(dataset, Dataset):\n",
    "            dataset = self._remove_unused_columns(dataset, description=description)\n",
    "\n",
    "        if isinstance(dataset, torch.utils.data.IterableDataset):\n",
    "            if self.args.world_size > 1:\n",
    "                dataset = IterableDatasetShard(\n",
    "                    dataset,\n",
    "                    batch_size=self.args.train_batch_size,\n",
    "                    drop_last=self.args.dataloader_drop_last,\n",
    "                    num_processes=self.args.world_size,\n",
    "                    process_index=self.args.process_index,\n",
    "                )\n",
    "\n",
    "            return SingleTaskDataLoader(\n",
    "                task,\n",
    "                dataset=dataset,\n",
    "                batch_size=self.args.per_device_train_batch_size,\n",
    "                collate_fn=self.data_collator,\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "\n",
    "        if description == \"training\":\n",
    "            self.train_dataset, dataset = dataset, self.train_dataset\n",
    "            sampler = self._get_train_sampler()\n",
    "            self.train_dataset, dataset = dataset, self.train_dataset\n",
    "            batch_size = self.args.train_batch_size\n",
    "        else:\n",
    "            sampler = self._get_eval_sampler(dataset)\n",
    "            batch_size = self.args.eval_batch_size\n",
    "\n",
    "        return SingleTaskDataLoader(\n",
    "            task,\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        return MultiTaskDataLoader({\n",
    "            task: self.get_single_task_dataloader(task, dataset, description=\"training\")\n",
    "            for task, dataset in self.train_dataset.items()\n",
    "        })\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        task_to_eval = eval_dataset[\"task\"]\n",
    "        return self.get_single_task_dataloader(task_to_eval, eval_dataset[task_to_eval], description=\"evaluation\")\n",
    "\n",
    "    def get_test_dataloader(self, test_dataset):\n",
    "        task_to_test = test_dataset[\"task\"]\n",
    "        return self.get_single_task_dataloader(task_to_test, test_dataset[task_to_test], description=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(tokenizer, labels, **kwargs):\n",
    "\n",
    "    meld_files = {\n",
    "        \"train\": kwargs[\"data_dir\"] + \"MELD/train_sent_emo.csv\", \n",
    "        \"validation\": kwargs[\"data_dir\"] + \"MELD/dev_sent_emo.csv\",\n",
    "        \"test\": kwargs[\"data_dir\"] + \"MELD/test_sent_emo.csv\"\n",
    "    }\n",
    "    \n",
    "    emorynlp_files = {\n",
    "        \"train\": kwargs[\"data_dir\"] + \"EMORYNLP/train.csv\",\n",
    "        \"validation\": kwargs[\"data_dir\"] + \"EMORYNLP/dev.csv\",\n",
    "        \"test\": kwargs[\"data_dir\"] + \"EMORYNLP/test.csv\"\n",
    "    }\n",
    "    \n",
    "    datasets = {\"MELD\": load_dataset(\"csv\", data_files=meld_files),\n",
    "                \"EmoryNLP\": load_dataset(\"csv\", data_files=emorynlp_files)}\n",
    "    \n",
    "    def encode_label(example, labels):\n",
    "        for task, label in labels.items():\n",
    "            if task == \"Speaker\":\n",
    "                example[task] = label.str2int(example[task]) \\\n",
    "                    if example[task] in label.names else label.str2int(\"Others\")\n",
    "            else:\n",
    "                example[task] = label.str2int(example[task])\n",
    "        return example\n",
    "\n",
    "    for name, dataset in datasets.items():\n",
    "        datasets[name] = dataset.map(lambda e: encode_label(e, labels[name]))\n",
    "\n",
    "    def add_context(example, idx, dataset, labels):\n",
    "        example[\"Past\"] = \"\"\n",
    "        example[\"Future\"] = \"\"\n",
    "\n",
    "        if example[\"Utterance_ID\"] != 0:\n",
    "            i = 1\n",
    "            while idx - i >= 0:\n",
    "                past = dataset[idx - i]\n",
    "                past_speaker = labels[\"Speaker\"].int2str(past[\"Speaker\"])\n",
    "                past_utterance = past[\"Utterance\"]\n",
    "                if kwargs[\"speaker_in_context\"]:\n",
    "                    example[\"Past\"] = past_speaker + \":\" + past_utterance + \" \" + example[\"Past\"]\n",
    "                else:\n",
    "                    example[\"Past\"] = past_utterance + \" \" + example[\"Past\"]\n",
    "                if past[\"Utterance_ID\"] == 0 or i >= kwargs[\"num_past_utterances\"]:\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "        if idx + 1 < len(dataset) and dataset[idx + 1][\"Utterance_ID\"] != 0:\n",
    "            i = 1\n",
    "            while idx + i < len(dataset):\n",
    "                future = dataset[idx + i]\n",
    "                future_speaker = labels[\"Speaker\"].int2str(future[\"Speaker\"])\n",
    "                future_utterance = future[\"Utterance\"]\n",
    "                if kwargs[\"speaker_in_context\"]:\n",
    "                    example[\"Future\"] += \" \" + future_speaker + \":\" + future_utterance\n",
    "                else:\n",
    "                    example[\"Future\"] += \" \" + future_utterance\n",
    "                i += 1\n",
    "                if idx + i < len(dataset) and dataset[idx + i][\"Utterance_ID\"] == 0 \\\n",
    "                    or i >= kwargs[\"num_future_utterances\"]:\n",
    "                    break\n",
    "\n",
    "        return example\n",
    "\n",
    "    for name, dataset in datasets.items():\n",
    "        for split, ds in dataset.items():\n",
    "            dataset[split] = ds.map(lambda e, i: add_context(e, i, ds, labels[name]), with_indices=True)\n",
    "\n",
    "    def tokenize(example, add_past, add_future):\n",
    "        if add_past:\n",
    "            return tokenizer(example[\"Past\"], example[\"Utterance\"])\n",
    "        elif add_future:\n",
    "            return tokenizer(example[\"Utterance\"], example[\"Future\"])\n",
    "        else:\n",
    "            return tokenizer(example[\"Utterance\"])\n",
    "        \n",
    "    for name, dataset in datasets.items():\n",
    "        cx_datasets = {}\n",
    "        cx_datasets[\"with_past\"] = dataset.map(\n",
    "            lambda e: tokenize(e, add_past=True, add_future=False), batched=True)\n",
    "        cx_datasets[\"with_future\"] = dataset.map(\n",
    "            lambda e: tokenize(e, add_past=False, add_future=True), batched=True)\n",
    "        cx_datasets[\"no_context\"] = dataset.map(\n",
    "            lambda e: tokenize(e, add_past=False, add_future=False), batched=True)\n",
    "\n",
    "        tasks = list(labels[name].keys())\n",
    "        for cx in cx_datasets:\n",
    "            cols_to_keep = [\"input_ids\", \"attention_mask\"] + tasks\n",
    "            cols_to_remove = [c for c in cx_datasets[cx][\"train\"].column_names if c not in cols_to_keep]\n",
    "            cx_datasets[cx] = cx_datasets[cx].remove_columns(cols_to_remove)\n",
    "            task_datasets = {}\n",
    "            for task in tasks:\n",
    "                label = labels[name][task]\n",
    "                ds = cx_datasets[cx]\n",
    "                ds = ds.cast_column(task, label)\n",
    "                ds = ds.remove_columns([t for t in tasks if t != task])\n",
    "                ds = ds.rename_column(task, \"labels\")\n",
    "                ds.set_format()\n",
    "                task_datasets[task] = (ds, label)\n",
    "            cx_datasets[cx] = task_datasets\n",
    "        datasets[name] = cx_datasets\n",
    "\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_datasets(datasets, **kwargs):\n",
    "    for name, cx_datasets in datasets.items():\n",
    "        task_datasets = {}\n",
    "        for split in [\"train\", \"validation\", \"test\"]:\n",
    "            task_datasets[split] = {}\n",
    "            for cx in cx_datasets:\n",
    "                if cx == \"with_past\" and kwargs[\"num_past_utterances\"] == 0:\n",
    "                    continue\n",
    "                elif cx == \"with_future\" and kwargs[\"num_future_utterances\"] == 0:\n",
    "                    continue\n",
    "                elif cx == \"no_context\" and kwargs[\"num_past_utterances\"] + kwargs[\"num_future_utterances\"] > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    for task, (ds, _) in cx_datasets[cx].items():\n",
    "                        if split == \"train\" and task not in kwargs[\"training\"]:\n",
    "                            continue\n",
    "                        if task not in task_datasets[split]:\n",
    "                            task_datasets[split][task] = ds[split]\n",
    "                        else:\n",
    "                            ds_to_concat = [task_datasets[split][task], ds[split]]\n",
    "                            task_datasets[split][task] = concatenate_datasets(ds_to_concat)\n",
    "\n",
    "        train_dataset = task_datasets[\"train\"]\n",
    "        eval_dataset = task_datasets[\"validation\"]\n",
    "        test_dataset = task_datasets[\"test\"]\n",
    "\n",
    "        eval_dataset[\"task\"] = kwargs[\"evaluation\"]\n",
    "        test_dataset[\"task\"] = kwargs[\"evaluation\"]\n",
    "        \n",
    "        datasets[name] = {\"train\": train_dataset, \"validation\": eval_dataset, \"test\": test_dataset}\n",
    "    \n",
    "    train_dataset = datasets[\"MELD\"][\"train\"]\n",
    "    train_dataset[\"EmoryNLP\"] = datasets[\"EmoryNLP\"][\"train\"][\"Emotion\"]\n",
    "    eval_dataset = datasets[\"MELD\"][\"validation\"]\n",
    "    test_dataset = datasets[\"MELD\"][\"test\"]\n",
    "\n",
    "    return train_dataset, eval_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"MELD\": \n",
    "    {\n",
    "        \"Speaker\": ClassLabel(\n",
    "            num_classes=7,\n",
    "            names=[\"Chandler\", \"Joey\", \"Monica\", \"Rachel\", \"Ross\", \"Phoebe\", \"Others\"]),\n",
    "        \"Emotion\": ClassLabel(\n",
    "            num_classes=7,\n",
    "            names=[\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]),\n",
    "        \"Sentiment\": ClassLabel(\n",
    "            num_classes=3,\n",
    "            names=[\"positive\", \"neutral\", \"negative\"])\n",
    "    },\n",
    "    \"EmoryNLP\": \n",
    "    {   \n",
    "        \"Speaker\": ClassLabel(\n",
    "            num_classes=7,\n",
    "            names=[\"Chandler\", \"Joey\", \"Monica\", \"Rachel\", \"Ross\", \"Phoebe\", \"Others\"]),\n",
    "        \"Emotion\": ClassLabel(\n",
    "            num_classes=7,\n",
    "            names=[\"Sad\", \"Mad\", \"Scared\", \"Powerful\", \"Peaceful\", \"Joyful\", \"Neutral\"])\n",
    "    }\n",
    "}\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(kwargs[\"checkpoint\"])\n",
    "datasets = preprocess(tokenizer, labels, **kwargs)\n",
    "train_dataset, eval_dataset, test_dataset = prepare_datasets(datasets, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tasks = {task: labels[\"MELD\"][task] for task in labels[\"MELD\"] if task in kwargs[\"training\"]}\n",
    "tasks[\"EmoryNLP\"] = labels[\"EmoryNLP\"][\"Emotion\"]\n",
    "task_models = {\n",
    "    task: AutoModelForSequenceClassification.from_pretrained(\n",
    "        kwargs[\"checkpoint\"], num_labels=label.num_classes)\n",
    "    for task, label in tasks.items()\n",
    "}\n",
    "multi_task_model = MultiTaskModel.from_task_models(task_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not kwargs[\"do_train\"]:\n",
    "    multi_task_model.load_state_dict(\n",
    "        torch.load(kwargs[\"model_file\"], map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_task_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=kwargs[\"train_dir\"],\n",
    "    seed=kwargs[\"seed\"],\n",
    "    overwrite_output_dir=True,\n",
    "    label_names=[\"labels\"],\n",
    "    learning_rate=kwargs[\"learning_rate\"],\n",
    "    num_train_epochs=kwargs[\"epoch\"],\n",
    "    per_device_train_batch_size=kwargs[\"batch_size\"],\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\"\n",
    ")\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    multi_task_model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if kwargs[\"do_train\"]:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = trainer.predict(test_dataset)\n",
    "f1 = pred.metrics['test_f1']\n",
    "print(\"Weighted F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_labels = labels[\"MELD\"][kwargs[\"evaluation\"]].int2str(pred.predictions.argmax(axis=-1))\n",
    "true_labels = labels[\"MELD\"][kwargs[\"evaluation\"]].int2str(pred.label_ids)\n",
    "inputs = tokenizer.batch_decode(test_dataset[kwargs[\"evaluation\"]][\"input_ids\"])\n",
    "f = open(kwargs[\"output_file\"], \"w\")\n",
    "f.write(\"Input\\tPredicted\\tTrue\\n\")\n",
    "f.write(\"\\n\".join([\"\\t\".join([input, pred_label, true_label]) \n",
    "                    for input, pred_label, true_label \n",
    "                    in zip(inputs, pred_labels, true_labels)]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p results\n",
    "import json\n",
    "\n",
    "f = open(kwargs[\"result_file\"], \"a+\")\n",
    "f.write(json.dumps(kwargs))\n",
    "f.write(\"\\nWeighted F1: {}\\n\".format(f1))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(kwargs[\"result_file\"])\n",
    "files.download(kwargs[\"output_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!cp \"/content/outputs/multi_task_model/pytorch_model.bin\" \"/content/gdrive/MyDrive/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_sample(ds_test, tokenizer, idx=None, max_tokens=512):\n",
    "    while True:\n",
    "        if idx is None:\n",
    "            idx_ = np.random.randint(0, len(ds_test))\n",
    "        else:\n",
    "            idx_ = idx\n",
    "        random_sample = ds_test[idx_]\n",
    "        input_ids, attention_mask, labelid = (\n",
    "            random_sample[\"input_ids\"],\n",
    "            random_sample[\"attention_mask\"],\n",
    "            random_sample[\"labels\"],\n",
    "        )\n",
    "        break\n",
    "\n",
    "    decoded = tokenizer.decode(input_ids)\n",
    "\n",
    "    input_ids = torch.tensor(input_ids).view(-1, len(input_ids))\n",
    "    attention_mask = torch.tensor(attention_mask).view(-1, len(attention_mask))\n",
    "    labelid = torch.tensor(labelid).view(-1, 1)\n",
    "\n",
    "    return idx_, input_ids, attention_mask, labelid, decoded\n",
    "\n",
    "def return_coeffs(\n",
    "    tokenizer,\n",
    "    input_ids,\n",
    "    attentions,\n",
    "    BATCH_IDX=0,\n",
    "    LAYER=-1,\n",
    "    QUERY_TOKEN_IDX=0,\n",
    "    annoying_char=\"Ġ\",\n",
    "):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[BATCH_IDX].tolist())\n",
    "    QUERY_TOKEN = tokens[QUERY_TOKEN_IDX].split(annoying_char)[-1]\n",
    "\n",
    "    coeffs = (\n",
    "        attentions[LAYER][BATCH_IDX].cpu().detach().numpy().sum(axis=0)[QUERY_TOKEN_IDX]\n",
    "    )\n",
    "    coeffs /= coeffs.sum()\n",
    "\n",
    "    idx_token_coeffs = [\n",
    "        (idx, token.split(annoying_char)[-1], coeffs[idx])\n",
    "        for idx, token in enumerate(tokens)\n",
    "    ]\n",
    "\n",
    "    assert len(coeffs) == len(tokens) == len(idx_token_coeffs)\n",
    "\n",
    "    return QUERY_TOKEN, coeffs, tokens, idx_token_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "ds_test = test_dataset[kwargs[\"evaluation\"]]\n",
    "idx, input_ids, attention_mask, labelid, decoded = get_random_sample(\n",
    "  ds_test, tokenizer\n",
    ")\n",
    "\n",
    "_, _, _, labelid_speaker, _ = get_random_sample(\n",
    "  test_dataset[\"Speaker\"], tokenizer, idx=idx\n",
    ")\n",
    "\n",
    "pprint.pprint(f\"{decoded}\")\n",
    "print()\n",
    "\n",
    "outputs = multi_task_model(\n",
    "  **{\"input_ids\": input_ids.to(device), \"attention_mask\": attention_mask.to(device)},\n",
    "  labels=labelid.to(device),\n",
    "  output_attentions=True,\n",
    "  output_hidden_states=True,\n",
    "  task=\"Emotion\"\n",
    ")\n",
    "\n",
    "#outputs_speaker = multi_task_model(\n",
    "#  **{\"input_ids\": input_ids.to(device), \"attention_mask\": attention_mask.to(device)},\n",
    "#  labels=labelid.to(device),\n",
    "#  output_attentions=True,\n",
    "#  output_hidden_states=True,\n",
    "#  task=\"Speaker\"\n",
    "#)\n",
    "\n",
    "attentions = outputs.attentions\n",
    "pred = labels[\"MELD\"][\"Emotion\"].int2str(int(outputs.logits.argmax().cpu().numpy()))\n",
    "truth = labels[\"MELD\"][\"Emotion\"].int2str(int(labelid[0][0].numpy()))\n",
    "\n",
    "#pred_speaker = labels[\"MELD\"][\"Speaker\"].int2str(int(outputs_speaker.logits.argmax().cpu().numpy()))\n",
    "#truth_speaker = labels[\"MELD\"][\"Speaker\"].int2str(int(labelid_speaker[0][0].numpy()))\n",
    "\n",
    "pprint.pprint(f\"data_idx: {idx}\")\n",
    "pprint.pprint(f\"pred: {pred}\")\n",
    "pprint.pprint(f\"truth: {truth}\")\n",
    "#pprint.pprint(f\"pred: {pred_speaker}\")\n",
    "#pprint.pprint(f\"truth: {truth_speaker}\")\n",
    "pprint.pprint(f\"number of tokens in the input: {input_ids.shape[1]}\")\n",
    "print()\n",
    "\n",
    "QUERY_TOKEN, coeffs, tokens, idx_token_coeffs = return_coeffs(\n",
    "    tokenizer, input_ids, attentions, LAYER=-1, QUERY_TOKEN_IDX=0\n",
    ")\n",
    "\n",
    "top_10 = sorted(idx_token_coeffs, key=lambda x: -x[2])[:10]\n",
    "print(top_10)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DmpSNfMXW8AW",
    "Y7LZ98xqW3sk",
    "3vu9ks2HWodt"
   ],
   "name": "D2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13f124717e3a46898289ae9aa2bad191": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "434f1308b48340a9bbebcc9884ffcbe1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "471a922b05c048bd90359a03a72de82e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d63a455ce5046d491b0291fc4d536a4",
      "placeholder": "​",
      "style": "IPY_MODEL_13f124717e3a46898289ae9aa2bad191",
      "value": "Downloading builder script: "
     }
    },
    "5d63a455ce5046d491b0291fc4d536a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c0d8b279c0e442c8e241ce0822156e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b02467e58ce04062b3f87628b8291b43",
      "max": 2059,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_99cda41eb08843d780f5825386be0d05",
      "value": 2059
     }
    },
    "70ef47f9c26c4585ab9a85e43dee48a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99cda41eb08843d780f5825386be0d05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a24c1c7229ce4b54be0df61081ecde00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_434f1308b48340a9bbebcc9884ffcbe1",
      "placeholder": "​",
      "style": "IPY_MODEL_e74fb905d22f46c1bcde03739301e7d0",
      "value": " 5.27k/? [00:00&lt;00:00, 152kB/s]"
     }
    },
    "b02467e58ce04062b3f87628b8291b43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e74fb905d22f46c1bcde03739301e7d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5876590064446d98b0e78d28e0e5cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_471a922b05c048bd90359a03a72de82e",
       "IPY_MODEL_6c0d8b279c0e442c8e241ce0822156e0",
       "IPY_MODEL_a24c1c7229ce4b54be0df61081ecde00"
      ],
      "layout": "IPY_MODEL_70ef47f9c26c4585ab9a85e43dee48a6"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C3iC3Yq8_nQP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "978396ed-938b-4576-8c30-5059476a7709",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001B[?25l\r\u001B[K     |█                               | 10 kB 31.1 MB/s eta 0:00:01\r\u001B[K     |██                              | 20 kB 37.9 MB/s eta 0:00:01\r\u001B[K     |███                             | 30 kB 38.4 MB/s eta 0:00:01\r\u001B[K     |████                            | 40 kB 25.8 MB/s eta 0:00:01\r\u001B[K     |█████                           | 51 kB 22.7 MB/s eta 0:00:01\r\u001B[K     |██████                          | 61 kB 25.9 MB/s eta 0:00:01\r\u001B[K     |███████                         | 71 kB 23.1 MB/s eta 0:00:01\r\u001B[K     |████████                        | 81 kB 24.3 MB/s eta 0:00:01\r\u001B[K     |█████████                       | 92 kB 26.1 MB/s eta 0:00:01\r\u001B[K     |██████████                      | 102 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |███████████                     | 112 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |████████████                    | 122 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |█████████████                   | 133 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |██████████████                  | 143 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |███████████████                 | 153 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |████████████████▏               | 163 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |█████████████████▏              | 174 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |██████████████████▏             | 184 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |███████████████████▏            | 194 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |████████████████████▏           | 204 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▏          | 215 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |██████████████████████▏         | 225 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |███████████████████████▏        | 235 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▏       | 245 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▏      | 256 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▏     | 266 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▏    | 276 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▏   | 286 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▏  | 296 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▏ | 307 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▏| 317 kB 26.8 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 325 kB 26.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001B[K     |████████████████████████████████| 212 kB 71.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.1 MB 46.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "\u001B[K     |████████████████████████████████| 77 kB 8.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "\u001B[K     |████████████████████████████████| 136 kB 75.6 MB/s \n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001B[K     |████████████████████████████████| 127 kB 74.2 MB/s \n",
      "\u001B[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001B[K     |████████████████████████████████| 271 kB 70.9 MB/s \n",
      "\u001B[?25hCollecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001B[K     |████████████████████████████████| 94 kB 4.0 MB/s \n",
      "\u001B[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001B[K     |████████████████████████████████| 144 kB 76.9 MB/s \n",
      "\u001B[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001B[0m\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001B[K     |████████████████████████████████| 4.0 MB 33.4 MB/s \n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001B[K     |████████████████████████████████| 596 kB 62.0 MB/s \n",
      "\u001B[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "\u001B[K     |████████████████████████████████| 895 kB 78.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001B[K     |████████████████████████████████| 6.6 MB 59.9 MB/s \n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###util.py"
   ],
   "metadata": {
    "id": "r7GZnxDS9-Tt",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from os.path import abspath, dirname\n",
    "\n",
    "\n",
    "def get_root_dir():\n",
    "    result = dirname(abspath(__file__))\n",
    "    src = \"/src\"\n",
    "    if result.endswith(src):\n",
    "        result = result[:-len(src) + 1]\n",
    "    return result"
   ],
   "metadata": {
    "id": "8KZ_6sXQ96ZU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###load_data.py"
   ],
   "metadata": {
    "id": "an_J_wFTXD97",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2020 CLTL Leolani\n",
    "Copyright (c) 2021 Junyin Chen\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import util\n",
    "\n",
    "MELD_SPEAKER = [\"Chandler\", \"Joey\", \"Monica\", \"Rachel\", \"Ross\", \"Phoebe\"]\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seed to a fixed value.\n",
    "\n",
    "    Set everything to be deterministic\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def get_emotion2id(dataset: str) -> Tuple[dict, dict]:\n",
    "    \"\"\"Get a dict that converts string class to number.\"\"\"\n",
    "\n",
    "    if dataset == \"MELD\":\n",
    "        # MELD has 7 classes\n",
    "        emotions = [\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"]\n",
    "        emotion2id = {emotion: idx for idx, emotion in enumerate(emotions)}\n",
    "        id2emotion = {val: key for key, val in emotion2id.items()}\n",
    "    return emotion2id, id2emotion\n",
    "\n",
    "\n",
    "def get_speaker2id(dataset: str) -> Tuple[dict, dict]:\n",
    "    \"\"\"Get a dict that converts string class to number.\"\"\"\n",
    "\n",
    "    if dataset == \"MELD\":\n",
    "        # MELD has 7 classes\n",
    "        speakers = [\n",
    "            \"CHANDLER\",\n",
    "            \"JOEY\",\n",
    "            \"MONICA\",\n",
    "            \"RACHEL\",\n",
    "            \"ROSS\",\n",
    "            \"PHOEBE\",\n",
    "            \"OTHERS\",\n",
    "        ]\n",
    "        speaker2id = {speaker: idx for idx, speaker in enumerate(speakers)}\n",
    "        id2speaker = {val: key for key, val in speaker2id.items()}\n",
    "    return speaker2id, id2speaker\n",
    "\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset=\"MELD\",\n",
    "            split=\"train\",\n",
    "            field=\"emotion\",\n",
    "            num_past_utterances=0,\n",
    "            num_future_utterances=0,\n",
    "            model_checkpoint=\"roberta-base\",\n",
    "            directory=util.get_root_dir() + \"data/\",\n",
    "            up_to=False,\n",
    "            seed=0\n",
    "    ):\n",
    "        \"\"\"Initialize emotion recognition in conversation text modality dataset class.\"\"\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.directory = directory\n",
    "        self.split = split\n",
    "        self.field = field\n",
    "        self.num_past_utterances = num_past_utterances\n",
    "        self.num_future_utterances = num_future_utterances\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.emotion2id, self.id2emotion = get_emotion2id(self.dataset)\n",
    "        self.speaker2id, self.id2speaker = get_speaker2id(self.dataset)\n",
    "        self.up_to = up_to\n",
    "        self.seed = seed\n",
    "\n",
    "        self._load_from_raw()\n",
    "        self._string2tokens()\n",
    "\n",
    "    def _load_from_raw(self):\n",
    "        if self.dataset in [\"MELD\"]:\n",
    "            # Get the full file path\n",
    "            raw_path = os.path.join(self.directory, self.dataset, self.split + \"_sent_emo.csv\")\n",
    "            # Load csv to pandas Dataframe\n",
    "            raw_data = pd.read_csv(raw_path)\n",
    "            # Load each field\n",
    "            self.dialog_group = {}\n",
    "            self.emotion = {}\n",
    "            self.speaker_emotion = {}\n",
    "            for _, row in raw_data.iterrows():\n",
    "                utterance_id = '{}_{}_{}_{}'.format(row['Dialogue_ID'],\n",
    "                                                    row['Utterance_ID'],\n",
    "                                                    row['Season'],\n",
    "                                                    row['Episode'])\n",
    "                dialog_id = '{}_{}_{}'.format(row['Dialogue_ID'], row['Season'], row['Episode'])\n",
    "                if dialog_id not in self.dialog_group:\n",
    "                    self.dialog_group[dialog_id] = []\n",
    "                self.dialog_group[dialog_id].append(utterance_id)\n",
    "                self.emotion[utterance_id] = row['Emotion']\n",
    "                utterance = row[\"Utterance\"]\n",
    "                speaker = \"OTHERS\"\n",
    "                if row[\"Speaker\"] in MELD_SPEAKER:\n",
    "                    speaker = row[\"Speaker\"].upper()\n",
    "\n",
    "                self.speaker_emotion[utterance_id] = {\"utterance\": speaker + \": \" + utterance,\n",
    "                                                      \"clean\": utterance,\n",
    "                                                      \"emotion\": row['Emotion'],\n",
    "                                                      \"speaker\": speaker,\n",
    "                                                      }\n",
    "        else:\n",
    "            raise ValueError(f\"{self.dataset} is not MELD\")\n",
    "\n",
    "\n",
    "    def _create_input(\n",
    "            self, diaids, num_past_utterances, num_future_utterances\n",
    "    ):\n",
    "        \"\"\"Create an input which will be an input to RoBERTa.\"\"\"\n",
    "\n",
    "        args = {\n",
    "            \"diaids\": diaids,\n",
    "            \"num_past_utterances\": num_past_utterances,\n",
    "            \"num_future_utterances\": num_future_utterances,\n",
    "        }\n",
    "\n",
    "        logging.debug(f\"arguments given: {args}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint, use_fast=True)\n",
    "        max_model_input_size = tokenizer.max_model_input_sizes[self.model_checkpoint]\n",
    "        num_truncated = 0\n",
    "\n",
    "        inputs = []\n",
    "        for diaid in tqdm(diaids):\n",
    "            ues = [\n",
    "                self.speaker_emotion[uttid]\n",
    "                for uttid in self.dialog_group[diaid]\n",
    "            ]\n",
    "\n",
    "            num_tokens = [len(tokenizer(ue[\"utterance\"])[\"input_ids\"]) for ue in ues]\n",
    "\n",
    "            for idx, ue in enumerate(ues):\n",
    "                if ue[\"emotion\"] not in list(self.emotion2id.keys()):\n",
    "                    continue\n",
    "\n",
    "                emotion = self.emotion2id[ue[\"emotion\"]]\n",
    "                speaker = self.speaker2id[ue[\"speaker\"]]\n",
    "\n",
    "                indexes = [idx]\n",
    "                indexes_past = [\n",
    "                    i for i in range(idx - 1, idx - num_past_utterances - 1, -1)\n",
    "                ]\n",
    "                indexes_future = [\n",
    "                    i for i in range(idx + 1, idx + num_future_utterances + 1, 1)\n",
    "                ]\n",
    "\n",
    "                offset = 0\n",
    "                if len(indexes_past) < len(indexes_future):\n",
    "                    for _ in range(len(indexes_future) - len(indexes_past)):\n",
    "                        indexes_past.append(None)\n",
    "                elif len(indexes_past) > len(indexes_future):\n",
    "                    for _ in range(len(indexes_past) - len(indexes_future)):\n",
    "                        indexes_future.append(None)\n",
    "\n",
    "                for i, j in zip(indexes_past, indexes_future):\n",
    "                    if i is not None and i >= 0:\n",
    "                        indexes.insert(0, i)\n",
    "                        offset += 1\n",
    "                        if (\n",
    "                                sum([num_tokens[idx_] for idx_ in indexes])\n",
    "                                > max_model_input_size\n",
    "                        ):\n",
    "                            del indexes[0]\n",
    "                            offset -= 1\n",
    "                            num_truncated += 1\n",
    "                            break\n",
    "                    if j is not None and j < len(ues):\n",
    "                        indexes.append(j)\n",
    "                        if (\n",
    "                                sum([num_tokens[idx_] for idx_ in indexes])\n",
    "                                > max_model_input_size\n",
    "                        ):\n",
    "                            del indexes[-1]\n",
    "                            num_truncated += 1\n",
    "                            break\n",
    "\n",
    "                utterances = [ues[idx_][\"utterance\"] for idx_ in indexes]\n",
    "                clean = [ues[idx_][\"clean\"] for idx_ in indexes]\n",
    "\n",
    "                if num_past_utterances == 0 and num_future_utterances == 0:\n",
    "                    assert len(utterances) == 1\n",
    "                    final_utterance = clean[0]\n",
    "\n",
    "                elif num_past_utterances > 0 and num_future_utterances == 0:\n",
    "                    if len(utterances) == 1:\n",
    "                        final_utterance = \"</s></s>\" + clean[-1]\n",
    "                    else:\n",
    "                        final_utterance = (\n",
    "                                \" \".join(utterances[:-1]) + \"</s></s>\" + clean[-1]\n",
    "                        )\n",
    "\n",
    "                elif num_past_utterances == 0 and num_future_utterances > 0:\n",
    "                    if len(utterances) == 1:\n",
    "                        final_utterance = clean[0] + \"</s></s>\"\n",
    "                    else:\n",
    "                        final_utterance = (\n",
    "                                clean[0] + \"</s></s>\" + \" \".join(utterances[1:])\n",
    "                        )\n",
    "\n",
    "                elif num_past_utterances > 0 and num_future_utterances > 0:\n",
    "                    if len(utterances) == 1:\n",
    "                        final_utterance = \"</s></s>\" + clean[0] + \"</s></s>\"\n",
    "                    else:\n",
    "                        final_utterance = (\n",
    "                                \" \".join(utterances[:offset])\n",
    "                                + \"</s></s>\"\n",
    "                                + clean[offset]\n",
    "                                + \"</s></s>\"\n",
    "                                + \" \".join(utterances[offset + 1:])\n",
    "                        )\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "                input_ids_attention_mask = tokenizer(final_utterance)\n",
    "                input_ids = input_ids_attention_mask[\"input_ids\"]\n",
    "                attention_mask = input_ids_attention_mask[\"attention_mask\"]\n",
    "\n",
    "                label = emotion\n",
    "                if self.field == \"speaker\":\n",
    "                    label = speaker\n",
    "\n",
    "                input_ = {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"label\": label,\n",
    "                }\n",
    "\n",
    "                inputs.append(input_)\n",
    "\n",
    "        logging.info(f\"number of truncated utterances: {num_truncated}\")\n",
    "        return inputs\n",
    "\n",
    "    def _string2tokens(self):\n",
    "        \"\"\"Convert string to (BPE) tokens.\"\"\"\n",
    "        logging.info(f\"converting utterances into tokens ...\")\n",
    "\n",
    "        diaids = sorted(list(self.dialog_group.keys()))\n",
    "\n",
    "        set_seed(self.seed)\n",
    "        random.shuffle(diaids)\n",
    "\n",
    "        if self.up_to:\n",
    "            logging.info(f\"Using only the first {self.up_to} dialogues ...\")\n",
    "            diaids = diaids[: self.up_to]\n",
    "\n",
    "        logging.info(f\"creating input utterance data ... \")\n",
    "        self.inputs_ = self._create_input(\n",
    "            diaids=diaids,\n",
    "            num_past_utterances=self.num_past_utterances,\n",
    "            num_future_utterances=self.num_future_utterances,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs_[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_)"
   ],
   "metadata": {
    "id": "Waxtsj0H8H4R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###MTL/data.py"
   ],
   "metadata": {
    "id": "DmpSNfMXW8AW",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class SingleTaskDataLoader:\n",
    "    def __init__(self, task, **kwargs):\n",
    "        self.task = task\n",
    "        self.data_loader = DataLoader(**kwargs)\n",
    "        self.batch_size = self.data_loader.batch_size\n",
    "        self.dataset = self.data_loader.dataset\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_loader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task\"] = self.task\n",
    "            yield batch\n",
    "    \n",
    "class MultiTaskDataLoader:\n",
    "    def __init__(self, task_data_loaders):\n",
    "        self.task_data_loaders = task_data_loaders\n",
    "        self.dataset = [None] * sum([len(dl.dataset) for dl in task_data_loaders.values()])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return sum([len(dl) for dl in self.task_data_loaders.values()])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        task_choices = []\n",
    "        for task, dl in self.task_data_loaders.items():\n",
    "            task_choices.extend([task] * len(dl))\n",
    "        task_choices = np.array(task_choices)\n",
    "        np.random.shuffle(task_choices)\n",
    "        for task in task_choices:\n",
    "            yield next(iter(self.task_data_loaders[task]))"
   ],
   "metadata": {
    "id": "brYBpgKb5GiG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###MTL/model.py"
   ],
   "metadata": {
    "id": "Y7LZ98xqW3sk",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class MultiTaskModel(PreTrainedModel):\n",
    "    def __init__(self, encoder, task_models):\n",
    "        super(MultiTaskModel, self).__init__(PretrainedConfig())\n",
    "        self.encoder = encoder\n",
    "        self.task_models = nn.ModuleDict(task_models)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_task_models(cls, task_models):\n",
    "        shared_encoder = None\n",
    "        for model in task_models.values():\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n",
    "            else:\n",
    "                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n",
    "        return cls(shared_encoder, task_models)\n",
    "                  \n",
    "    @staticmethod\n",
    "    def get_encoder_attr_name(model):\n",
    "        model_name = model.__class__.__name__\n",
    "        if model_name.startswith('Bert'):\n",
    "            return 'bert'\n",
    "        elif model_name.startswith('Roberta'):\n",
    "            return 'roberta'\n",
    "        elif model_name.startswith('Albert'):\n",
    "            return 'albert'\n",
    "        else:\n",
    "            raise ValueError('Unsupported model: {}'.format(model_name))\n",
    "        \n",
    "    def forward(self, task, input_ids, attention_mask, **kwargs):\n",
    "        model = self.task_models[task]\n",
    "        return model(input_ids, attention_mask, **kwargs)"
   ],
   "metadata": {
    "id": "BXWpdb1j5LSD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###MLT/train.py"
   ],
   "metadata": {
    "id": "3vu9ks2HWodt",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class MultiTaskTrainer(Trainer):\n",
    "\n",
    "    def get_single_task_dataloader(self, task, dataset, description):\n",
    "        if description == \"training\" and self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        elif description == \"evaluation\" and dataset is None and self.eval_dataset is None:\n",
    "            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
    "\n",
    "        if is_datasets_available() and isinstance(dataset, datasets.Dataset):\n",
    "            dataset = self._remove_unused_columns(dataset, description=description)\n",
    "\n",
    "        if isinstance(dataset, torch.utils.data.IterableDataset):\n",
    "            if self.args.world_size > 1:\n",
    "                dataset = IterableDatasetShard(\n",
    "                    dataset,\n",
    "                    batch_size=self.args.train_batch_size,\n",
    "                    drop_last=self.args.dataloader_drop_last,\n",
    "                    num_processes=self.args.world_size,\n",
    "                    process_index=self.args.process_index,\n",
    "                )\n",
    "\n",
    "            return SingleTaskDataLoader(\n",
    "                task,\n",
    "                dataset=dataset,\n",
    "                batch_size=self.args.per_device_train_batch_size,\n",
    "                collate_fn=self.data_collator,\n",
    "                num_workers=self.args.dataloader_num_workers,\n",
    "                pin_memory=self.args.dataloader_pin_memory,\n",
    "            )\n",
    "        \n",
    "        if description == \"training\":\n",
    "            self.train_dataset, dataset = dataset, self.train_dataset\n",
    "            sampler = self._get_train_sampler()\n",
    "            self.train_dataset, dataset = dataset, self.train_dataset\n",
    "            batch_size = self.args.train_batch_size\n",
    "        else:\n",
    "            sampler = self._get_eval_sampler(dataset)\n",
    "            batch_size = self.args.eval_batch_size\n",
    "            \n",
    "        return SingleTaskDataLoader(\n",
    "            task,\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        return MultiTaskDataLoader({\n",
    "            task: self.get_single_task_dataloader(task, dataset, description=\"training\")\n",
    "            for task, dataset in self.train_dataset.items()\n",
    "        })\n",
    "        \n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        task_to_eval = eval_dataset[\"task\"]\n",
    "        return self.get_single_task_dataloader(task_to_eval, eval_dataset[task_to_eval], description=\"evaluation\")\n",
    "        \n",
    "    \n",
    "    def get_test_dataloader(self, test_dataset):\n",
    "        task_to_test = test_dataset[\"task\"]\n",
    "        return self.get_single_task_dataloader(task_to_test, test_dataset[task_to_test], description=\"test\")\n"
   ],
   "metadata": {
    "id": "xJyRj5EU5PTa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###pipeline.py"
   ],
   "metadata": {
    "id": "SPbm1rsbWaOg",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel, load_metric\n",
    "\n",
    "from transformers import Trainer\n",
    "from transformers import is_datasets_available\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
    "\n",
    "from load_data import TextDataset\n",
    "import util"
   ],
   "metadata": {
    "id": "-g1c9UWTWMPK",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "outputId": "a968d128-79a8-4221-c773-f44d4b90c632",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "args = vars(util.get_args())\n",
    "\n",
    "train_dataset = {\n",
    "        \"emotion\": TextDataset(\n",
    "            dataset=\"MELD\",\n",
    "            split=\"train\",\n",
    "            field=\"emotion\",\n",
    "            seed=args[\"seed\"],\n",
    "            directory=args[\"data_dir\"],\n",
    "            num_past_utterances=args[\"num_past_utterances\"],\n",
    "            num_future_utterances=args[\"num_future_utterances\"],\n",
    "        ),\n",
    "        \"speaker\": TextDataset(\n",
    "            dataset=\"MELD\",\n",
    "            split=\"train\",\n",
    "            field=\"speaker\",\n",
    "            seed=args[\"seed\"],\n",
    "            directory=args[\"data_dir\"],\n",
    "            num_past_utterances=args[\"num_past_utterances\"],\n",
    "            num_future_utterances=args[\"num_future_utterances\"],\n",
    "        )\n",
    "    }\n",
    "eval_dataset = {\"emotion\": TextDataset(\n",
    "      dataset=\"MELD\",\n",
    "      split=\"dev\",\n",
    "      field=\"emotion\",\n",
    "      seed=args[\"seed\"],\n",
    "      directory=args[\"data_dir\"],\n",
    "      num_past_utterances=args[\"num_past_utterances\"],\n",
    "      num_future_utterances=args[\"num_future_utterances\"],\n",
    "  ), \"speaker\": TextDataset(\n",
    "        dataset=\"MELD\",\n",
    "        split=\"dev\",\n",
    "        field=\"speaker\",\n",
    "        seed=args[\"seed\"],\n",
    "        directory=args[\"data_dir\"],\n",
    "        num_past_utterances=args[\"num_past_utterances\"],\n",
    "        num_future_utterances=args[\"num_future_utterances\"],\n",
    "    ), \"task\": args[\"evaluation\"]}\n",
    "\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "labels = {\n",
    "        \"speaker\": ClassLabel(num_classes=7,\n",
    "                              names=[\"Chandler\", \"Joey\", \"Monica\", \"Rachel\", \"Ross\", \"Phoebe\", \"Others\"]),\n",
    "        \"emotion\": ClassLabel(num_classes=7,\n",
    "                              names=[\"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", \"surprise\"])\n",
    "    }"
   ],
   "metadata": {
    "id": "0WPaSwEN5aET",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "outputId": "5ee21ef8-72aa-46a8-9ad8-3a434bc09525",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--seed SEED]\n",
      "                             [--num_past_utterances NUM_PAST_UTTERANCES]\n",
      "                             [--num_future_utterances NUM_FUTURE_UTTERANCES]\n",
      "                             [--data_dir DATA_DIR] [--do_train]\n",
      "                             [--evaluation EVALUATION] [--train_dir TRAIN_DIR]\n",
      "                             [--model_file MODEL_FILE] [--epoch EPOCH]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/junyinchen/Library/Jupyter/runtime/kernel-6c614347-4ef4-4d55-894b-622fe3216e25.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junyinchen/anaconda3/envs/UW-LING-573/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "single_task_models = {\n",
    "        task: AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=label.num_classes)\n",
    "        for task, label in labels.items()\n",
    "    }\n",
    "multi_task_model = MultiTaskModel.from_single_task_models(single_task_models)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329,
     "referenced_widgets": [
      "353a21777aec4812a671073b692a1b83",
      "17d9c509f0b748cabccef69dfc4f77fc",
      "c66a61718ce54e538c1b5c136fdb0d37",
      "fa1d06f2847f426db6ff7031c3d7672d",
      "f605ad256bc642e292b8a3e2e4636de1",
      "201cbf9d0ab843c4847e9b6d1f4adaea",
      "78ca362ac83b40529fcd961485b9d0f8",
      "a83709d0b4b84bbd8e30d711df975f89",
      "f37ae823c06a4c979a4af22ae8d52a64",
      "5c49c61f13af4a0e9d6e6f13add35650",
      "bdf91c66789a4e74a9f86db5b300c095"
     ]
    },
    "id": "Qd5BQJe6_255",
    "outputId": "c63d5c93-d1a1-4fdb-a298-d4eeec80e109",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "353a21777aec4812a671073b692a1b83"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../outputs/multi_task_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    label_names=[\"labels\"],\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  \n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    multi_task_model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "do_train = True\n",
    "\n",
    "if do_train:\n",
    "    trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898,
     "referenced_widgets": [
      "507fe904658f49ec9171c1813d92bb7c",
      "467b10c17ca6495ca58951b59bebceed",
      "ec24963e45814b55a86df901e16bf4ba",
      "7dbcbc49e8eb4180a16181f2a4666c38",
      "40152211717b471abe4befedceaec226",
      "1f0e57fb819f4732b89894af2614eabc",
      "af255ffc5dcc43d69920ef1d7c82d0af",
      "220a9514c0a741c4919a10d4ae432929",
      "4802a6591830422681136a696d92e71e",
      "adb6d09a07094e629a1fbbf6dae852ec",
      "b092b3dedc844bc290d739310f298c4c"
     ]
    },
    "id": "2gazbzYk_yKX",
    "outputId": "5d098c2d-8e3d-4a7c-8811-83d6c98bdd9b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 19978\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7494\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7494' max='7494' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7494/7494 10:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.930700</td>\n",
       "      <td>1.194585</td>\n",
       "      <td>0.577457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>1.299061</td>\n",
       "      <td>0.571411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.561700</td>\n",
       "      <td>1.338199</td>\n",
       "      <td>0.582055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1109\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "507fe904658f49ec9171c1813d92bb7c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving model checkpoint to ../outputs/multi_task_model/checkpoint-2498\n",
      "Configuration saved in ../outputs/multi_task_model/checkpoint-2498/config.json\n",
      "Model weights saved in ../outputs/multi_task_model/checkpoint-2498/pytorch_model.bin\n",
      "tokenizer config file saved in ../outputs/multi_task_model/checkpoint-2498/tokenizer_config.json\n",
      "Special tokens file saved in ../outputs/multi_task_model/checkpoint-2498/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1109\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../outputs/multi_task_model/checkpoint-4996\n",
      "Configuration saved in ../outputs/multi_task_model/checkpoint-4996/config.json\n",
      "Model weights saved in ../outputs/multi_task_model/checkpoint-4996/pytorch_model.bin\n",
      "tokenizer config file saved in ../outputs/multi_task_model/checkpoint-4996/tokenizer_config.json\n",
      "Special tokens file saved in ../outputs/multi_task_model/checkpoint-4996/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1109\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ../outputs/multi_task_model/checkpoint-7494\n",
      "Configuration saved in ../outputs/multi_task_model/checkpoint-7494/config.json\n",
      "Model weights saved in ../outputs/multi_task_model/checkpoint-7494/pytorch_model.bin\n",
      "tokenizer config file saved in ../outputs/multi_task_model/checkpoint-7494/tokenizer_config.json\n",
      "Special tokens file saved in ../outputs/multi_task_model/checkpoint-7494/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../outputs/multi_task_model/checkpoint-2498 (score: 1.1945850849151611).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "f1 = trainer.predict(test_dataset).metrics['test_f1']\n",
    "print(\"Weighted F1:\", f1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "CUM1PbI0__Nn",
    "outputId": "1bf45f0e-d3d3-49a4-8443-a3396ec3b7e0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2610\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='981' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [327/327 01:23]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Weighted F1: 0.6040721346150377\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DmpSNfMXW8AW",
    "Y7LZ98xqW3sk",
    "3vu9ks2HWodt"
   ],
   "name": "D2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "353a21777aec4812a671073b692a1b83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17d9c509f0b748cabccef69dfc4f77fc",
       "IPY_MODEL_c66a61718ce54e538c1b5c136fdb0d37",
       "IPY_MODEL_fa1d06f2847f426db6ff7031c3d7672d"
      ],
      "layout": "IPY_MODEL_f605ad256bc642e292b8a3e2e4636de1"
     }
    },
    "17d9c509f0b748cabccef69dfc4f77fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_201cbf9d0ab843c4847e9b6d1f4adaea",
      "placeholder": "​",
      "style": "IPY_MODEL_78ca362ac83b40529fcd961485b9d0f8",
      "value": "Downloading: 100%"
     }
    },
    "c66a61718ce54e538c1b5c136fdb0d37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a83709d0b4b84bbd8e30d711df975f89",
      "max": 501200538,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f37ae823c06a4c979a4af22ae8d52a64",
      "value": 501200538
     }
    },
    "fa1d06f2847f426db6ff7031c3d7672d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c49c61f13af4a0e9d6e6f13add35650",
      "placeholder": "​",
      "style": "IPY_MODEL_bdf91c66789a4e74a9f86db5b300c095",
      "value": " 478M/478M [00:09&lt;00:00, 59.6MB/s]"
     }
    },
    "f605ad256bc642e292b8a3e2e4636de1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "201cbf9d0ab843c4847e9b6d1f4adaea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78ca362ac83b40529fcd961485b9d0f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a83709d0b4b84bbd8e30d711df975f89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f37ae823c06a4c979a4af22ae8d52a64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5c49c61f13af4a0e9d6e6f13add35650": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdf91c66789a4e74a9f86db5b300c095": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "507fe904658f49ec9171c1813d92bb7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_467b10c17ca6495ca58951b59bebceed",
       "IPY_MODEL_ec24963e45814b55a86df901e16bf4ba",
       "IPY_MODEL_7dbcbc49e8eb4180a16181f2a4666c38"
      ],
      "layout": "IPY_MODEL_40152211717b471abe4befedceaec226"
     }
    },
    "467b10c17ca6495ca58951b59bebceed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f0e57fb819f4732b89894af2614eabc",
      "placeholder": "​",
      "style": "IPY_MODEL_af255ffc5dcc43d69920ef1d7c82d0af",
      "value": "Downloading builder script: "
     }
    },
    "ec24963e45814b55a86df901e16bf4ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_220a9514c0a741c4919a10d4ae432929",
      "max": 2059,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4802a6591830422681136a696d92e71e",
      "value": 2059
     }
    },
    "7dbcbc49e8eb4180a16181f2a4666c38": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adb6d09a07094e629a1fbbf6dae852ec",
      "placeholder": "​",
      "style": "IPY_MODEL_b092b3dedc844bc290d739310f298c4c",
      "value": " 5.27k/? [00:00&lt;00:00, 166kB/s]"
     }
    },
    "40152211717b471abe4befedceaec226": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f0e57fb819f4732b89894af2614eabc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af255ffc5dcc43d69920ef1d7c82d0af": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "220a9514c0a741c4919a10d4ae432929": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4802a6591830422681136a696d92e71e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "adb6d09a07094e629a1fbbf6dae852ec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b092b3dedc844bc290d739310f298c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
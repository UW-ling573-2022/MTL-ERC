% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
\documentclass[11pt]{article}
\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\title{Multilingual Emotion Recognition in Conversation}

\author{Junyin Chen {} {} {} Hanshu Ding {} {} {} Zoe Fang {} {} {} Yifan Jiang \\
		\texttt{\{junyinc, hsding99, zoekfang, yfjiang\}@uw.edu} \\
        Department of Linguistics \\ University of Washington}

\begin{document}
\maketitle
\begin{abstract}
  TBD
\end{abstract}

\section{Introduction}
TBD

\section{Task Description}

\subsection{Primary Task}
\label{sect:primary_task}

Our primary task is emotion recognition in conversation (ERC) task on the text modality of the Multimodal EmotionLines Dataset (MELD). The dataset is in English and contains dialogues and utterances from TV series scripts . We will predict the emotion for each utterance from dialogues involving multiple speakers.

\subsubsection{Primary Dataset}

MELD \citep{poria-etal-2019-meld} \footnote{\url{https://affective-meld.github.io/}}, also kown as \textit{Multimodel Emotionlines Dataset}, is a multi-party emotional conversational database that is extended from Emotionlines dataset. 
Emotionlines \citep{hsu-etal-2018-emotionlines}\footnote{\url{http://doraemon.iis.sinica.edu.tw/emotionlines/index.html}} dataset is an emotion dialogue dataset with emotion labels for each utterance. The utterances are collected from Friends TV scripts and private Facebook messenger dialogues. Each utterance is labeled with one of Ekman’s six basic emotions plus the neutral emotion. MELD is an upgraded emotion dataset that contains about 13,000 utterances from 1,433 dialogues from only the TV-series Friends. Each utterance in MELD is annotated with emotion and sentiment labels, and encompasses audio, visual, and textual modalities. 
MELD splits the data into training, development, and testing set separately (1039 dialogues for training set, 114 dialogues for developing set and 280 dialogues for testing set). We use the testing set and corresponding gold standard annotation for analysis.

According to Poria, the speakers of these dialogues are categorized to 6 main characters and others. The utterances are distributed relatively evenly with respect to speakers (from 12\% to 16\%).  The emotion distribution for each character is also similar. 

We also use the EmoryNLP \citep{EmoryNLP} as an auxiliary dataset to facilitate the training. EmoryNLP is another multi-party emotional conversational database curated by Zahiri and Choi. The utterances are also collected from Friends TV show and are annotated with emotion and speaker labels by crowdsourced worker based on Willcox's feeling wheel. EmoryNLP splits the dataset into training, development, and testing set separately. We use the testing set and corresponding gold standard annotation for analysis. 

According to Zahiri and Choi, the distribution of all emotions in the corpus is not even. The two most dominant emotions, \textit{neutral} and \textit{joyful}, together yield over 50\% of the dataset.

\subsection{Adaptation Task}
\label{sect:adaptation_task}

Our adaptation task is to adapt our model to Chinese dialogues in the Multi-party Dialogue Dataset (MPDD). Other dimensions for this task remain the same as the primary task.

\subsubsection{Dataset}

Multi-party Dialogue Dataset (MPDD) \citep{chen-etal-2020-mpdd} \footnote{\url{http://nlg.csie.ntu.edu.tw/nlpresource/MPDD}} is a Chinese emotional conversational dataset. The dataset contains a total of 25, 548 utterances from 4, 142 dialogues, which are collected from five TV series scripts from \url{www.juban108.com}. Each uttrance is annmotated with three types of labels: emotion, relation, and target listener. In particular, the emotion labels are consistent with those in the Emotionlines dataset.


\section{System Overview}
\label{sec:overview}

\subsection{Design}

Instead of appending the correspondent speaker tag with the utterance for training, we integrate the ideology of having a multi-task deep neural network that shares the lower layer across multiple single-sentence and pairwise text classification tasks. 

With the benefit of multi-task deep neural network, we enable our model to gain context awareness with multiple context related tags, such as speaker, past and future utterance.

\subsection{Methodology}

We want to determine the emotion of a specific utterance in a dialogue group $M$, which contains multiple speakers in speaker group $S$. The dialogue group $M$ can be expressed as a list of vectors: $dialogue = [x_1, x_2, ..., x_{n-1}, x_n]$, where each utterance contains multiple words. The correspondent speaker list can be expressed as another list of vectors: $speaker = [s_1, s_1, ..., s_2, s_3]$, where $s_n$ is in the speaker group $S$. Since this is a supervised setup, we will utilize the manually labeled emotion label $e_n$ corresponding to each utterance $x_n$ in the dialogue group $M$.

The simplest solution is to have a function $f$ that takes each utterance $x_t$ as input and output the correct label $e_t$. For our setup, we decide on a multi-tasking function $f$ to output correspondent speaker $s_t$, as we want to take the speaker into account. Furthermore, we add past utterance $[x_1, x_2, ..., x_{t-1}]$ and even future utterance $[x_{t+1}, x_{t+2}, ..., x_n]$, as we anticipate adding more context around the utterance for analysis will further improve the result.


\subsection{Algorithm}

For each task in the multi-task deep neural network, we use the pretrained "Roberta-base" model \citet{liu-etal-2019-multi}. We choose BERT like algorithms as they both have simple structure, and support more than one segment for tokenization. We choose to use RoBERTa as the main algorithm for training, facilitating result comparison with results listed in \citet{kim-2021-emoberta}. Even though pre-trained BERT and RoBERTa models do not expect more than two segments as inputs, both Kim et al. and us show that having more than two segments improves evaluation results.

We will have either two or three segments, if we want to include both past and future utterances.


\section{Approach}
\label{sec:approach}

\subsection{Problem Statement}
MELD provides information on the speaker and turn ID (in the dialogue) of each utterance. We want to take these two factors into account and build a model that learns emotion prediction based on the context and/or speaker information.


\subsection{Multi-Task Deep Neural Network}

We chose to build a multi-task deep neural network. The shared layer is a RoBERTa base encoder. For the different task heads, we set the main task to be emotion recognition on the MELD data-set, which is also the data-set we run our final evaluation on. The auxiliary task trains parallel to the main task, the point of the auxiliary task is for the model to learn relevant information in addition to the main task. In our model, we have two options for auxiliary task. One of the options is speaker classification, since past literature on emotion recognition in conversation suggest that the models might perform better when they learn speaker-specific features, and we think there might be correlation between each speaker and a certain emotion distribution. The other option for auxiliary task is that we can use data augmentation and run the same emotion classification task, but on additional data-set (EmoryNLP). We can also have more than one auxiliary task, so we run both speaker classification and data augmentation.

\subsection{Input Preprocessing}
Other than the model architecture, our approach also involves specific techniques in input preprocessing. The default input is an utterance in a dialogue, given the turn ID, which is the sequential number in the dialogue, and the speaker information. Since we want to experiment on the impact of speaker information on model performance, we can add a speaker classification auxiliary task, or we can do this in input preprocessing, where we provide the speaker information by concatenating them to the utterance, so that the speaker can potentially be a hint for the model to learn. Another way to preprocess input is to provide context of the utterance. We can provide a certain amount of past or future utterances of the utterance that we want to predict. The caveat of training a MT-DNN on top of RoBERTa is that BERT-like models are previously trained with inputs no longer than two sentences, where each two sentence has a BOS \textit{<s>} token and an EOS \textit{</s>} token. For that reason, when we concatenate more than one past utterance to the to-predict utterance, we strip the EOS token of the past utterances and keep only one that exists between the context and the to-predict utterance. Algorithm~\ref{context} shows our method of adding context and speaker to an utterance.
\begin{algorithm}
\caption{Add All Past Utterance(s)}\label{euclid}
\begin{algorithmic}[1]
\State \textbf{Given} an $Uttr$ and its $idx$ in dialogue
\If {$idx \neq 0$}
    \State $i \gets 1$
    \While {$idx - i \geq 0$}
        \State $Uttr_{past} \gets Data_{idx-i}$
        \State $Speaker_{past} \gets Uttr_{Past_{speaker}}$
        \State $str \gets Speaker_{past} + Uttr_{Past}$
        \State $Context \gets Context + str$
        \State $i \gets i + 1$
    \EndWhile
\EndIf
\State $Uttr \gets Context + EOS + Uttr$
\end{algorithmic}
\label{context}
\end{algorithm}
\subsection{Evaluation}

Both tasks will be evaluated using weighted F1 to account for the imbalance of the dataset. We will also evaluate the error rate between predicted the true emotion labels that are labelled manually by the dataset curators. 


\section{Results}
\label{sec:results}

\begin{table}[hbt]
  \centering
  \begin{tabular}{c|c|c}
    past utterance & future utterance & weighted F1 \\
    \hline
    0 & 0 & 60.17 \\
    \hline
    0 & 6 & 61.96 \\
    \hline
    6 & 0 & \hl{62.46} \\
  \end{tabular}
  \caption{Weighted F1 when number of past or future utterances are added as context. Auxiliary task = Speaker Classification. Speaker in input = True.}
\end{table}

\begin{table}[hbt]
  \centering
  \begin{tabular}{c|c|c}
    speaker task & speaker in input & weighted F1 \\
    \hline
    Yes & No & 63.75 \\
    \hline
    Yes & Yes & 62.92 \\
    \hline
    No & Yes & 62.60 \\
    \hline
    No & No & \hl{64.74} \\
  \end{tabular}
  \caption{Weighted F1 when auxiliary task is speaker classification and/or speaker information is concatenated to input utterance. Number of Past Utterances = 10}
\end{table}

Using our evaluation metric, the model currently yields a highest 64.74 percent weighted F1 score. Table 1 shows the results before we add the EmoryNLP dataset, and we have speaker classification task as the auxiliary task. Table 1 shows that adding only past utterances yields the best result. Table 2 shows the results when we set the one of the auxiliary tasks to emotion prediction, while keeping the second auxiliary task of speaker classification optional. We also experiment with the option of having speaker in input. 

\begin{table}[hbt]
  \centering
  \begin{tabular}{c|c|c|c}
    emotion & true count & total count & accuracy \\
    \hline
    neutral & 1026 & 1256 & 0.8169 \\
    \hline
    joy & 247 & 402 & 0.6144 \\
    \hline
    anger & 156 & 345 & 0.4522  \\
    \hline
    surprise & 173 & 281 & 0.6157 \\
    \hline
    sadness & 66 & 208 & 0.3173 \\
    \hline
    disgust & 5 & 68 & 0.0735  \\
    \hline
    fear & 1 & 50 & 0.02 \\
  \end{tabular}
  \caption{Prediction accuracy for each emotion in model with speaker classification task}
\end{table}

\begin{table}[hbt]
  \centering
  \begin{tabular}{c|c|c|c}
    emotion & true count & total count & accuracy \\
    \hline
    neutral & 985 & 1256 & 0.7842 \\
    \hline
    joy & 263 & 402 & 0.6542 \\
    \hline
    anger & 173 & 345 & 0.5014  \\
    \hline
    surprise & 180 & 281 & 0.6429 \\
    \hline
    sadness & 76 & 208 & 0.3654\\
    \hline
    disgust  & 13 & 68 & 0.1912  \\
    \hline
    fear & 11 & 50 & 0.22 \\
  \end{tabular}
  \caption{Prediction accuracy for each emotion in best-performing model}
\end{table}

\begin{figure}
\begin{tikzpicture}
      \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        ymajorgrids=true,
        %
        ylabel=Accuracy Rate,
        ymin=0,
        ytick={0,0.25,0.50,0.75,1.00},
        enlargelimits=auto,
        %
        xlabel= Emotions,
        xtick={1,2,...,7},
        xticklabels={{neutral},{joy}, {anger},{surprise},{sadness},{disgust},{feat}},
        x tick label style={rotate=30,anchor=north east},
        ]
        \addplot[color=red,mark=x]
          plot coordinates {
            (1,0.8169)
            (2,0.6144)
            (3,0.4522)
            (4,0.6157)
            (5,0.3173)
            (6,0.0735)
            (7,0.02)
          };
          \addlegendentry{D2}
                
      \addplot[color=blue,mark=*]
          plot coordinates {
            (1,0.7842)
            (2,0.6542)
            (3,0.5014)
            (4,0.6406)
            (5,0.3654)
            (6,0.1912)
            (7,0.22)
          };
          \addlegendentry{D3}
        \node [above] at (axis cs:  1,  0.7842) {$0.7842$};
      \end{axis}
    \end{tikzpicture}
\caption{D2 Accuracy vs D3 Accuracy}
\label{fig:stats}
\end{figure}


The prediction output and accuracy is shown as above in Table 3 and 4. A major trend we see from the tables is that the prediction for "neutral" has the highest accuracy compared to other emotions. Before data augmentation, the accuracy for the prediction of specific emotion decreases as the quantity of learning instances for the corresponding emotion decreases. After data augmentation, distributional bias in the data-set is not as patent. Specifically, we see a great improvement in "fear" prediction.

\section{Discussion}

\subsection{Pre-trained model decision}

Using RoBERTA-base pre-trained model, we achieve similar result illustrated in Kim et al. However, using RoBERTa-large or RoBERTa-full pre-trained model used by Kim et al., weighted F1 score reduced by as much as 30 percent, regardless of number of utterances or auxiliary tasks included in the training dataset. We choose to stick with RoBERTa-base model for Delivery 3.

\subsection{Number of past and future utterance.}

Adding future utterance alone does not greatly improve the result. The result of adding future utterance alone is similar of having no future or past utterance. Adding both past and future utterance, preforms almost the same as adding only past utterance. Due to GPU memory size, we choose to only add past utterance in Delivery 3 and increase number of include past utterance to 10.

\subsection{Qualitative Analysis}

We did a qualitative analysis with on 10 correctly and 10 incorrectly classified random sample from the test split for Delivery 2. For all the incorrectly classified random sample, the speaker tags are all incorrectly classified. For the 10 correctly classified random sample, 40 percent of the speaker tags are correctly classified. This shows that having speaker tag does help improve the result, but not as substantial as we hopped. 

{\small

\begin{quote}
<s>\hl{Rachel}\hl{:}Oh, that sounds great.Others:\hl{How} does that \hl{sound}?Others:Well, I’ve got a project for you that’s a lot more related to fashion.Others\hl{:}Well, don’t think I haven’t noticed your potential.Rachel\hl{:}Oh, you got me.Others\hl{:}Eh.</s></s>Come on over here, sweetheart\hl{.}</s>

\end{quote}

A correctly classified example. Both the prediction and truth are 'neutral'. Both the predicted speaker and the true speaker are Others.

\begin{quote}

<s>\hl{Mon}ica:Is that too much to ask after six year\hl{?!}\hl{Mon}ica:I \hl{mean}, all I’m asking for is just a little emotio\hl{!} \hl{Ch}andler:And you’re upset because you didn’t make your \hl{best} friend cry?</s></s>I mean \hl{what}?</s>

\end{quote}

An incorrectly classified example. The prediction is 'surprise' while the truth is 'anger'. The predicted speaker is Rachel while the true speaker is Monica.}

We initially anticipate that the speaker tag will be utilized for sentiment classification. However, both the incorrectly and correctly classified example, the <s> token in the last layer mainly focus on punctuation marks. This phenomenon far deviates from Kim et al.'s result, that <s> token mainly focus on the speaker in correctly classified examples. This proves that separating speaker and utterance for speaker detection does not benefit the model to utilize speaker context for sentiment analysis.\\

For Delivery 3, we did another qualitative analysis with on 10 correctly and 10 incorrectly classified random sample from the MELD test split.

{\small
\begin{quote}

<s>Joey:You know, I think I was sixteen. \hl{Monica}:Please, just \hl{a} little bit off the back. \hl{Ph}oe\hl{be}:I\'m still on "no." \hl{</s></s>Uh,} \hl{morning}. \hl{Do} \hl{you} guys think \hl{you} \hl{could} \hl{close} \hl{your} eyes \hl{for} \hl{just} a \hl{sec}\hl{?</s>}

\end{quote}

A correctly classified example when training with speaker context. Both the prediction and truth are 'neutral'. The predicted speaker is Monica, and the true speaker is Rachel.

\begin{quote}

\hl{<s>}\hl{Rachel}:It's not a purse! It's a shoulder bag\hl{.} \hl{Joey}:It looks like a women's purse\hl{.} \hl{Rachel}:\hl{No} \hl{Joey}, look. Trust me, all the men are wearing them in the spring catalog\hl{.} Look\hl{.} See look, \hl{Joey}:See look, \hl{</s></s>Exactly!} \hl{Unisex!</s>}

\end{quote}

An incorrectly classified example when training with speaker context. The prediction is 'joy' while the truth is 'neutral'. The predicted and the true speaker is Rachel.}

After using EmoryNLP for auxiliary emotion and speaker detection tasks, we notice an increase of accuracy in emotion detection task. However, the speaker contexts are mainly utilized by incorrect outputs. Only 5 of the correct samples utilize the speaker tag, where as all incorrect samples utilize the speaker tag. Both correct and incorrect samples have sixty percent accuracy of predicting the speaker. Based on the the new finding, we revised our hypothesis, that adding speaker contexts does not improve the emotion detection accuracy. We then train a new model without the speaker context.

{\small
\begin{quote}

\hl{<s>}You know, I think I was sixteen. Please, just a little bit off the back. \hl{Ph}oe\hl{be}:I\'m still on "no." \hl{</s></s>Uh,} \hl{morning}. \hl{Do} \hl{you} guys think \hl{you} \hl{could} \hl{close} \hl{your} eyes \hl{for} \hl{just} a \hl{sec}\hl{?</s>}

\end{quote}

A correctly classified example when training without speaker context. Both the prediction and truth are 'neutral'.

\begin{quote}

\hl{<s>}It's not a purse! It's a shoulder bag\hl{.} It looks like a women's purse\hl{.} \hl{No} Joey, look\hl{.} Trust me, all the men are wearing them in the spring catalog\hl{.} Look\hl{.} \hl{See} \hl{look}, See \hl{look,} \hl{</s></s>Exactly!} \hl{Unisex!</s>}

\end{quote}

An incorrectly classified example when training without speaker context. The prediction is 'joy' while the truth is 'neutral'.}

We re-examine the selected random samples with the new models, and the result is the same. We then randomly selected 10 correct samples using the new model and process them with the old model. The old model failed to correctly predict three of the utterances.

Our new model, with an weighted F1 of 64.7\%, suggesting speaker context is unnecessary when adding additional dataset. 



\section{Conclusion}
\label{sect:conclusion}

TBD

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology, custom}

\end{document}
